Importación de librerías
-------------------------------------------------------------------
import lightgbm as lgb
from lightgbm.callback import early_stopping
import numpy as np
import pandas as pd
import os
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
import plotly.express as px
from itertools import islice
import re
from sklearn.metrics import mean_squared_log_error
import itertools
import warnings
warnings.filterwarnings('ignore')
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
from IPython.display import display_html
-------------------------------------------------------------------
1.	import lightgbm as lgb: Importa la librería LightGBM (Light Gradient Boosting Machine) que se usa para crear modelos de machine learning basados en árboles de decisión.
2.	from lightgbm.callback import early_stopping: Importa la función early_stopping de LightGBM, que se usa para evitar sobreajuste deteniendo el entrenamiento si el modelo no mejora después de un número de rondas.
3.	import numpy as np: Importa NumPy, una librería para trabajar con arrays y operaciones matemáticas.
4.	import pandas as pd: Importa Pandas, una librería para el análisis de datos y manipulación de estructuras de datos tabulares.
5.	import os: Importa la librería os para interactuar con el sistema operativo, en este caso, para manejar archivos y directorios.
6.	from sklearn.model_selection import StratifiedKFold: Importa StratifiedKFold de sklearn, una técnica para dividir los datos en K partes de manera estratificada.
7.	from sklearn.metrics import accuracy_score: Importa la función accuracy_score de sklearn.metrics para evaluar la precisión del modelo.
8.	import plotly.express as px: Importa Plotly Express como px, que es una librería para generar gráficos interactivos.
9.	from itertools import islice: Importa islice de itertools, que permite crear iteradores que toman solo una parte de una lista o secuencia.
10.	import re: Importa la librería re para trabajar con expresiones regulares.
11.	from sklearn.metrics import mean_squared_log_error: Importa la métrica mean_squared_log_error, utilizada para medir la precisión de la predicción en problemas de regresión, en este caso, calculando el RMSLE (Root Mean Squared Logarithmic Error).
12.	import itertools: Importa la librería itertools, útil para trabajar con iteradores y combinaciones.
13.	import warnings: Importa la librería warnings para manejar las advertencias de Python.
14.	warnings.filterwarnings('ignore'): Desactiva las advertencias en el código.
15.	from matplotlib.colors import ListedColormap, LinearSegmentedColormap: Importa los métodos ListedColormap y LinearSegmentedColormap de matplotlib.colors para crear mapas de colores personalizados.
16.	from IPython.display import display_html: Importa la función display_html de IPython.display, que permite mostrar código HTML en el entorno Jupyter.
-------------------------------------------------------------------

Cargar y mostrar los archivos CSV
-------------------------------------------------------------------
for dirname, _, filenames in os.walk('/content/data_'):
    for filename in filenames:        
        print(os.path.join(dirname, filename))
        no_ext = f'{os.path.splitext(filename)[0]}_df'
        no_ext = no_ext.replace(" ", "_")
        no_ext = no_ext.replace("-", "_")
        globals()[no_ext] = pd.read_csv(os.path.join(dirname, filename))
-------------------------------------------------------------------
•	os.walk('/content/data_'): Recorre el directorio /content/data_ y sus subdirectorios, devolviendo las rutas de los archivos.
•	for filename in filenames: Itera sobre todos los archivos encontrados en el directorio.
•	print(os.path.join(dirname, filename)): Imprime la ruta completa del archivo CSV.
•	no_ext = f'{os.path.splitext(filename)[0]}_df': Obtiene el nombre del archivo sin su extensión y le añade el sufijo _df.
•	no_ext = no_ext.replace(" ", "_").replace("-", "_"): Sustituye los espacios y guiones por guiones bajos en el nombre del archivo.
•	globals()[no_ext] = pd.read_csv(os.path.join(dirname, filename)): Lee el archivo CSV y lo convierte en un DataFrame de Pandas, guardándolo con el nombre no_ext (una variable global).

Función para mostrar información sobre el DataFrame
-------------------------------------------------------------------
def df_info(df, name="Default"): 
    print(clr.S+f"=== {name} ==="+clr.E)
    if not hasattr(df, 'shape'):
        print(clr.S + "Shape:" + clr.E, format(df.shape[0], ","), format(df.shape[1], ","))
    else:
        print(clr.S + "Shape:" + clr.E, df.shape)
    print(clr.S+f"Valores faltantes:"+clr.E, format(df.isna().sum().sum(), ","), "total de datapoints perdidos.")
    print(clr.S+"Columnas:"+clr.E, list(df.columns), "\n")
    display_html(df.tail())
    print("\n")
-------------------------------------------------------------------
•	df_info(df, name="Default"): Función que muestra información sobre el DataFrame df, como su tamaño (filas y columnas), valores faltantes y las primeras filas.
•	format(df.shape[0], ","): Muestra el número de filas con formato de miles.
•	df.isna().sum().sum(): Cuenta la cantidad total de valores faltantes en el DataFrame.
•	display_html(df.tail()): Muestra las últimas filas del DataFrame en formato HTML en un entorno Jupyter.

Resumen del DataFrame
-------------------------------------------------------------------
def summary(df):
    print(f'data shape: {df.shape}')
    summ = pd.DataFrame(df.dtypes, columns=['data type'])
    summ['#missing'] = df.isnull().sum().values 
    summ['%missing'] = df.isnull().sum().values / len(df) * 100
    summ['#unique'] = df.nunique().values
    desc = pd.DataFrame(df.describe(include='all').transpose())
    
    if 'min' in desc.columns:
        summ['min'] = desc['min'].values
    else:
        summ['min'] = 'N/A'

    if 'max' in desc.columns:
        summ['max'] = desc['max'].values
    else:
        summ['max'] = 'N/A'
    
    summ['first value'] = df.iloc[0].values if len(df) > 0 else 'N/A'
    summ['second value'] = df.iloc[1].values if len(df) > 1 else 'N/A'
    summ['third value'] = df.iloc[2].values if len(df) > 2 else 'N/A'

    display_html(summ)
    print("\n")
-------------------------------------------------------------------
•	summary(df): Esta función genera un resumen detallado del DataFrame df, incluyendo el número de valores faltantes, la cantidad de valores únicos, y las estadísticas descriptivas.
•	df.dtypes: Obtiene los tipos de datos de las columnas.
•	df.isnull().sum().values: Cuenta los valores nulos por columna.
•	df.nunique().values: Muestra el número de valores únicos por columna.
•	df.describe(include='all'): Muestra las estadísticas descriptivas, como el mínimo, máximo, promedio, etc.
•	summ['first value'], summ['second value'], summ['third value']: Muestra los primeros tres valores de las columnas si existen.
-------------------------------------------------------------------

Conversión de fecha y agregación de transacciones
-------------------------------------------------------------------
transactions_df['date'] = pd.to_datetime(transactions_df['date'])
daily_transactions = transactions_df.groupby('date').transactions.sum().reset_index()
daily_transactions = daily_transactions.sort_values(by='date', ascending=True)
daily_transactions['day_of_week'] = daily_transactions['date'].dt.day_name()
daily_transactions['hover_text'] = daily_transactions['date'].dt.strftime('%Y-%m-%d') + " (" + daily_transactions['day_of_week'] + "): " + daily_transactions['transactions'].astype(str)
-------------------------------------------------------------------
•	transactions_df['date'] = pd.to_datetime(transactions_df['date']): Convierte la columna date de transactions_df en formato de fecha y hora.
•	groupby('date').transactions.sum(): Agrupa las transacciones por fecha y calcula la suma de transacciones para cada día.
•	reset_index(): Reinicia el índice del DataFrame después de la agregación.
•	daily_transactions.sort_values(by='date', ascending=True): Ordena las transacciones diarias por fecha.
•	daily_transactions['day_of_week'] = daily_transactions['date'].dt.day_name(): Agrega una columna con el nombre del día de la semana.
•	daily_transactions['hover_text']: Crea una columna personalizada con texto para mostrar al pasar el mouse sobre los puntos en un gráfico.

Graficando las transacciones
-------------------------------------------------------------------
fig = px.line(daily_transactions, x='date', y='transactions', title='Transactions Trend Across All Stores by Date', hover_name='hover_text')
fig.show()
•	px.line(): Crea un gráfico de líneas con Plotly para mostrar las transacciones a lo largo del tiempo.
•	fig.show(): Muestra el gráfico interactivo.

Procesamiento de datos (Filtrado y creación de características)
-------------------------------------------------------------------
copy_train = train_df.copy()
copy_train['sales'] = copy_train['sales'].round()
copy_train = copy_train[(copy_train['sales'] >= 0) & (copy_train['sales'] <= 10000)]
fig = px.histogram(copy_train, x='sales')
fig.update_layout(title='Ventas agregadas y redondeadas', xaxis_title='Número de ventas', yaxis_title='Frecuencia')
fig.show()
-------------------------------------------------------------------
•	copy_train = train_df.copy(): Hace una copia del DataFrame train_df.
•	copy_train['sales'] = copy_train['sales'].round(): Redondea los valores de la columna sales al número entero más cercano.
•	**`copy_train = copy_train[(copy_train['sales'] >= 0) & (copy_train['sales'] <=
10000)]`**: Filtra las ventas que están entre 0 y 10,000.
•	px.histogram(copy_train, x='sales'): Crea un histograma de las ventas usando Plotly.
•	fig.show(): Muestra el histograma interactivo.


Función para agregar características relacionadas con las fechas
-------------------------------------------------------------------
def add_datepart(df, fldname, drop=True, time=True, sin_cos=False):
    df = df.copy()
    fld = df[fldname]
    df[fldname + '_year'] = fld.dt.year
    df[fldname + '_month'] = fld.dt.month
    df[fldname + '_day'] = fld.dt.day
    df[fldname + '_weekday'] = fld.dt.weekday
    df[fldname + '_dayofweek'] = fld.dt.dayofweek
    df[fldname + '_dayofyear'] = fld.dt.dayofyear
    if time:
        df[fldname + '_hour'] = fld.dt.hour
        df[fldname + '_minute'] = fld.dt.minute
        df[fldname + '_second'] = fld.dt.second
    if sin_cos:
        df[fldname + '_sin_day'] = np.sin(2 * np.pi * df[fldname + '_dayofyear'] / 365.25)
        df[fldname + '_cos_day'] = np.cos(2 * np.pi * df[fldname + '_dayofyear'] / 365.25)
    if drop: 
        df.drop(fldname, axis=1, inplace=True)
    return df

Explicación de la función add_datepart:
-------------------------------------------------------------------
•	add_datepart: La función agrega características relacionadas con la fecha, como el año, mes, día, día de la semana, hora, etc., a partir de una columna de tipo datetime.
•	fld.dt.year, fld.dt.month, fld.dt.weekday, etc.: Extrae información específica de la columna de fecha.
•	sin_cos=True: Si se establece como verdadero, se agregan columnas de seno y coseno para representar de manera circular las fechas (útil para el modelado).

Preprocesamiento de la variable date en el DataFrame
-------------------------------------------------------------------
train_df = add_datepart(train_df, 'date', time=True, sin_cos=True)
test_df = add_datepart(test_df, 'date', time=True, sin_cos=True)
•	train_df = add_datepart(train_df, 'date', time=True, sin_cos=True): Aplica la función add_datepart al DataFrame de entrenamiento (train_df) para agregar características adicionales basadas en la columna date. Se agregan las características de hora, minuto, segundo y características sinusoidales de las fechas.
•	test_df = add_datepart(test_df, 'date', time=True, sin_cos=True): Aplica la misma función al DataFrame de prueba (test_df).

Análisis de correlación
-------------------------------------------------------------------
corr = train_df.corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
fig = px.imshow(corr, color_continuous_scale='RdBu', zmin=-1, zmax=1)
fig.update_layout(title="Correlación de características")
fig.show()
-------------------------------------------------------------------
•	train_df.corr(): Calcula la matriz de correlación para todas las características numéricas del DataFrame train_df.
•	mask = np.triu(np.ones_like(corr, dtype=bool)): Crea una máscara triangular superior para no mostrar los valores de correlación duplicados.
•	px.imshow(corr, color_continuous_scale='RdBu', zmin=-1, zmax=1): Crea un gráfico de mapa de calor de la correlación de las características utilizando Plotly. Los valores se normalizan en el rango [-1, 1].
•	fig.update_layout(title="Correlación de características"): Establece el título del gráfico.
•	fig.show(): Muestra el gráfico.

Preprocesamiento de datos de entrada
-------------------------------------------------------------------
X = train_df.drop(columns=['target_column_name']) # Aquí se debe usar el nombre de la columna objetivo
y = train_df['target_column_name'] # Aquí se debe usar el nombre de la columna objetivo
•	X = train_df.drop(columns=['target_column_name']): Crea el conjunto de características (X) eliminando la columna objetivo (target_column_name) del DataFrame de entrenamiento.
•	y = train_df['target_column_name']: Extrae la columna objetivo como y.

Validación cruzada con StratifiedKFold
-------------------------------------------------------------------
n_splits = 5
kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
•	StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42): Crea un objeto de validación cruzada estratificada con 5 particiones (folds). El parámetro shuffle=True asegura que los datos se barajen antes de dividirlos en los n_splits. random_state=42 establece la semilla para que los resultados sean reproducibles.

Creación y entrenamiento del modelo
-------------------------------------------------------------------
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9
}

model = lgb.LGBMRegressor(**params)
•	params: Configura los parámetros del modelo LightGBM.
o	'objective': 'regression': Define el tipo de problema como regresión (predicción continua).
o	'metric': 'rmse': Especifica la métrica a utilizar, en este caso, Root Mean Squared Error (RMSE).
o	'boosting_type': 'gbdt': Establece el tipo de algoritmo de boosting como Gradient Boosting Decision Tree (GBDT).
o	'num_leaves': 31: Número de hojas en los árboles.
o	'learning_rate': 0.05: Tasa de aprendizaje del modelo.
o	'feature_fraction': 0.9: Porcentaje de características a usar en cada iteración.
•	model = lgb.LGBMRegressor(**params): Crea un modelo de regresión utilizando LightGBM con los parámetros definidos.

Entrenamiento del modelo con validación cruzada
-------------------------------------------------------------------
for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):
    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]
    
    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=100, verbose=100)
    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)
    rmse = mean_squared_log_error(y_valid, y_pred) ** 0.5
    print(f'Fold {fold + 1} RMSE: {rmse}')
-------------------------------------------------------------------
•	for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):: Inicia un bucle para cada una de las divisiones de validación cruzada. kf.split(X, y) genera los índices de las divisiones de entrenamiento y validación.
•	X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]: Divide las características en entrenamiento y validación.
•	y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]: Divide la variable objetivo en entrenamiento y validación.
•	model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=100, verbose=100): Entrena el modelo con los datos de entrenamiento, validando en el conjunto de validación. early_stopping_rounds=100 significa que el entrenamiento se detendrá si no hay mejora durante 100 iteraciones.
•	y_pred = model.predict(X_valid, num_iteration=model.best_iteration): Realiza predicciones en el conjunto de validación utilizando el mejor número de iteraciones (según el early stopping).
•	rmse = mean_squared_log_error(y_valid, y_pred) ** 0.5: Calcula el RMSE (Root Mean Squared Error) a partir del Mean Squared Logarithmic Error.
•	print(f'Fold {fold + 1} RMSE: {rmse}'): Imprime el RMSE para cada fold de la validación cruzada.

Visualización de las predicciones
-------------------------------------------------------------------
fig = px.scatter(x=y_valid, y=y_pred, title="Predictions vs Actuals", labels={"x": "Actual", "y": "Predicted"})
fig.add_shape(type="line", x0=min(y_valid), y0=min(y_valid), x1=max(y_valid), y1=max(y_valid), line=dict(color="Red"))
fig.show()
•	px.scatter(x=y_valid, y=y_pred, title="Predictions vs Actuals", labels={"x": "Actual", "y": "Predicted"}): Crea un gráfico de dispersión para comparar las predicciones (y_pred) con los valores reales (y_valid).
•	fig.add_shape(...): Agrega una línea roja de 45 grados al gráfico, que representa la línea de igualdad entre las predicciones y los valores reales.
•	fig.show(): Muestra el gráfico.

Ajuste del modelo y predicción final
-------------------------------------------------------------------
model.fit(X, y)
test_predictions = model.predict(test_df.drop(columns=['target_column_name']))
•	model.fit(X, y): Entrena el modelo con todos los datos de entrenamiento (ahora usando todo X y y).
•	test_predictions = model.predict(test_df.drop(columns=['target_column_name'])): Realiza predicciones sobre el conjunto de prueba (test_df), excluyendo la columna objetivo.

Guardar las predicciones
-------------------------------------------------------------------
submission = pd.DataFrame({"Id": test_df["Id"], "Predicted": test_predictions})
submission.to_csv('submission.csv', index=False)
•	submission = pd.DataFrame({"Id": test_df["Id"], "Predicted": test_predictions}): Crea un DataFrame con las predicciones finales y los identificadores de los datos de prueba.
•	submission.to_csv('submission.csv', index=False): Guarda el DataFrame con las predicciones en un archivo CSV llamado submission.csv.

Conclusión
-------------------------------------------------------------------
Este es un desglose completo del código. El flujo general es:
1.	Cargar y preprocesar los datos.
2.	Crear nuevas características derivadas de las fechas.
3.	Entrenar un modelo de LightGBM utilizando validación cruzada.
4.	Evaluar el rendimiento del modelo y visualizar las predicciones.
5.	Realizar predicciones sobre los datos de prueba y guardarlas en un archivo CSV.
